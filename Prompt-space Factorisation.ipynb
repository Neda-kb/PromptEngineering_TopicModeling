{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-22T12:33:23.359644Z",
     "start_time": "2026-01-22T12:33:22.471767Z"
    }
   },
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import Load_Data\n",
    "import LLM_setting\n",
    "import Chunking\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any , Sequence\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:33:24.115051Z",
     "start_time": "2026-01-22T12:33:24.112101Z"
    }
   },
   "cell_type": "code",
   "source": "_ = load_dotenv(find_dotenv())",
   "id": "11a35dcb7425ae3d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:28.824025Z",
     "start_time": "2026-01-22T12:34:28.789587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#LLM Setting\n",
    "model = \"openai/gpt-oss-120b\"\n",
    "temperature = 0\n",
    "llm_setting= LLM_setting.LLMSetting(MODEL=model, TEMPERATURE=temperature)\n",
    "llm = LLM_setting.setting(llm_setting)"
   ],
   "id": "d2ee47026bc7151c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:31.150301Z",
     "start_time": "2026-01-22T12:34:29.852536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read Data\n",
    "file_path = Path(\"data/raw_filings/1326801_000132680125000017_meta-20241231.htm\")\n",
    "cfg = Load_Data.LoadData(file_path=file_path)\n",
    "docs = Load_Data.load_data(cfg)"
   ],
   "id": "20a62d34fcffdbbc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:31.998117Z",
     "start_time": "2026-01-22T12:34:31.823517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chunking\n",
    "chunk_size= 2500\n",
    "chunk_overlap = 250\n",
    "data_processing=Chunking.DataProcessing(file_path,chunk_size,chunk_overlap)\n",
    "doc_chunk =Chunking.chunks(data_processing)"
   ],
   "id": "9f4b9afdfc136412",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:34.593238Z",
     "start_time": "2026-01-22T12:34:34.591173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TOPICS_PER_CHUNK = 8\n",
    "KEYWORDS_PER_TOPIC = 8\n",
    "FINAL_K_TOPICS = 10"
   ],
   "id": "33a5818e3ab03d46",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:35.444559Z",
     "start_time": "2026-01-22T12:34:35.441904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text=(\n",
    "                \"We experienced strong net sales growth driven by iPhone and Services. \"\n",
    "                \"Supply constraints eased. Foreign exchange impacted revenue. \"\n",
    "                \"We repurchased shares and increased the dividend.\"\n",
    "            )\n",
    "output_json={\n",
    "                \"topics\": [\n",
    "                    {\"label\": \"Product Revenue Drivers\", \"keywords\": [\"iPhone\", \"Services\", \"net sales\", \"growth\", \"revenue\", \"demand\", \"mix\", \"products\"]},\n",
    "                    {\"label\": \"Supply Chain Conditions\", \"keywords\": [\"supply\", \"constraints\", \"inventory\", \"availability\", \"production\", \"lead times\", \"logistics\", \"capacity\"]},\n",
    "                    {\"label\": \"Foreign Exchange Effects\", \"keywords\": [\"foreign exchange\", \"currency\", \"FX\", \"headwind\", \"translation\", \"rates\", \"impact\", \"revenue\"]},\n",
    "                    {\"label\": \"Capital Return Program\", \"keywords\": [\"share repurchase\", \"buyback\", \"dividend\", \"capital return\", \"shareholders\", \"authorization\", \"cash\", \"stock\"]},\n",
    "                    {\"label\": \"Operating Performance\", \"keywords\": [\"margin\", \"profitability\", \"operating income\", \"expenses\", \"costs\", \"efficiency\", \"performance\", \"results\"]},\n",
    "                    {\"label\": \"Market Conditions\", \"keywords\": [\"macroeconomic\", \"consumer\", \"competition\", \"pricing\", \"market\", \"uncertainty\", \"trends\", \"demand\"]},\n",
    "                    {\"label\": \"Guidance and Outlook\", \"keywords\": [\"outlook\", \"expectations\", \"guidance\", \"future\", \"forecast\", \"assumptions\", \"trend\", \"risks\"]},\n",
    "                    {\"label\": \"Shareholder Value\", \"keywords\": [\"value\", \"returns\", \"EPS\", \"share count\", \"capital allocation\", \"liquidity\", \"cash flow\", \"investment\"]},\n",
    "                ]\n",
    "            }"
   ],
   "id": "15ea528f97db4695",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:36.166588Z",
     "start_time": "2026-01-22T12:34:36.164765Z"
    }
   },
   "cell_type": "code",
   "source": "_JSON_OBJ_RE = re.compile(r\"\\{.*\\}\", flags=re.DOTALL)",
   "id": "5e2029a7eb354009",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:36.865640Z",
     "start_time": "2026-01-22T12:34:36.863234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Exemplar:\n",
    "    input_text: str\n",
    "    output_json: Dict[str, Any]"
   ],
   "id": "d4b6aed989cfe7b8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:37.490303Z",
     "start_time": "2026-01-22T12:34:37.488178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Delimiter:\n",
    "    open: str = \"<TEXT>\"\n",
    "    close: str = \"</TEXT>\""
   ],
   "id": "280f22deef4f7142",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:38.143192Z",
     "start_time": "2026-01-22T12:34:38.140767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class PromptComponents:\n",
    "    role: Optional[str]\n",
    "    instruction: str\n",
    "    rubric: Optional[str]\n",
    "    exemplars: Sequence[Exemplar]\n",
    "    delimiter: Delimiter"
   ],
   "id": "73464de74a892f55",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:38.891188Z",
     "start_time": "2026-01-22T12:34:38.886546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class PromptVariant:\n",
    "    variant_id: str\n",
    "    components: PromptComponents\n",
    "def default_instruction() -> str:\n",
    "    return \"Identify and label the main topics in this corpus.\"\n",
    "\n",
    "def default_role_financial_analyst() -> str:\n",
    "    return \"You are a financial analyst.\"\n",
    "\n",
    "def default_rubric(topics_per_chunk: int, keywords_per_topic: int) -> str:\n",
    "\n",
    "    schema_line = 'Schema: {\"topics\":[{\"label\":str,\"keywords\":[str,...]}]}'\n",
    "    return (\n",
    "        \"Rubric:\\n\"\n",
    "        \"- Topics should be coherent, specific, and meaningful for financial reporting.\\n\"\n",
    "        \"- Topic labels should be short noun phrases (avoid generic labels like 'Other').\\n\"\n",
    "        \"- Topics should be mutually distinct (minimise keyword overlap).\\n\"\n",
    "        \"- Prefer substantive business/financial themes over boilerplate artifacts.\\n\"\n",
    "        \"- Avoid pure dates, identifiers, XBRL-like tags, or URL-like tokens as keywords.\\n\\n\"\n",
    "        \"Output constraints:\\n\"\n",
    "        \"Use ONLY the text inside the delimiter.\\n\"\n",
    "        f\"Return exactly {topics_per_chunk} topics.\\n\"\n",
    "        f\"Each topic must have exactly {keywords_per_topic} keywords.\\n\"\n",
    "        \"Output MUST be valid JSON only (no Markdown, no extra text).\\n\"\n",
    "        f\"{schema_line}\\n\"\n",
    "    )\n",
    "\n",
    "def default_exemplars(input_text:Any,output_json:Dict) -> List[Exemplar]:\n",
    "    return [\n",
    "        Exemplar(\n",
    "            input_text,\n",
    "            output_json,\n",
    "        )\n",
    "    ]\n",
    "def build_factorised_prompt(\n",
    "    variant: PromptVariant,\n",
    "    chunk_text: str,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> str:\n",
    "    c = variant.components\n",
    "\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # 1) role\n",
    "    if c.role:\n",
    "        parts.append(c.role.strip())\n",
    "\n",
    "    # 2) instruction\n",
    "    parts.append(c.instruction.strip())\n",
    "\n",
    "    # 3) rubric\n",
    "    if c.rubric:\n",
    "        parts.append(c.rubric.strip())\n",
    "\n",
    "    # 4) exemplars\n",
    "    if c.exemplars:\n",
    "        ex_lines: List[str] = [\"Exemplars:\"]\n",
    "        for i, ex in enumerate(c.exemplars, start=1):\n",
    "            ex_out = json.dumps(ex.output_json, ensure_ascii=False)\n",
    "            ex_lines.append(f\"[Example {i}]\")\n",
    "            ex_lines.append(\"Input:\")\n",
    "            ex_lines.append(ex.input_text.strip())\n",
    "            ex_lines.append(\"Output (JSON):\")\n",
    "            ex_lines.append(ex_out)\n",
    "            ex_lines.append(\"\")  # spacer\n",
    "        parts.append(\"\\n\".join(ex_lines).strip())\n",
    "\n",
    "    # 5) delimiter\n",
    "    d = c.delimiter\n",
    "    parts.append(f\"{d.open}\\n{chunk_text}\\n{d.close}\")\n",
    "\n",
    "    return \"\\n\\n\".join([p for p in parts if p and p.strip()])"
   ],
   "id": "9d28a716e0b73340",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:39.610860Z",
     "start_time": "2026-01-22T12:34:39.608851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _as_text(obj: Any) -> str:\n",
    "    if hasattr(obj, \"page_content\"):\n",
    "        return obj.page_content\n",
    "    if hasattr(obj, \"chunk\"):\n",
    "        ch = getattr(obj, \"chunk\", None)\n",
    "        if hasattr(ch, \"page_content\"):\n",
    "            return ch.page_content\n",
    "        return str(ch)\n",
    "    return str(obj)"
   ],
   "id": "a2147027c2dd968d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:40.316003Z",
     "start_time": "2026-01-22T12:34:40.313849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _parse_json_loose(s: str) -> Dict[str, Any]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = _JSON_OBJ_RE.search(s)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}"
   ],
   "id": "24f9f68ab682a4e2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:41.052821Z",
     "start_time": "2026-01-22T12:34:41.049595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_topics_for_chunk_factorised(\n",
    "    llm,\n",
    "    variant: PromptVariant,\n",
    "    chunk: Any,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    text = _as_text(chunk)\n",
    "\n",
    "    c = variant.components\n",
    "    rubric = c.rubric\n",
    "    if rubric is None:\n",
    "        rubric = default_rubric(topics_per_chunk, keywords_per_topic)\n",
    "        variant = PromptVariant(\n",
    "            variant_id=variant.variant_id,\n",
    "            components=PromptComponents(\n",
    "                role=c.role,\n",
    "                instruction=c.instruction,\n",
    "                rubric=rubric,\n",
    "                exemplars=c.exemplars,\n",
    "                delimiter=c.delimiter,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    prompt = build_factorised_prompt(\n",
    "        variant=variant,\n",
    "        chunk_text=text,\n",
    "        topics_per_chunk=topics_per_chunk,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    content = getattr(resp, \"content\", str(resp))\n",
    "    data = _parse_json_loose(content)\n",
    "\n",
    "    topics = data.get(\"topics\", [])\n",
    "    if not isinstance(topics, list):\n",
    "        return []\n",
    "\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "    for t in topics:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "        label = str(t.get(\"label\", \"\")).strip()\n",
    "        kws = t.get(\"keywords\", [])\n",
    "        if not label or not isinstance(kws, list):\n",
    "            continue\n",
    "        keywords = [str(k).strip() for k in kws if str(k).strip()]\n",
    "\n",
    "        if len(keywords) != keywords_per_topic:\n",
    "            continue\n",
    "        cleaned.append({\"label\": label, \"keywords\": keywords})\n",
    "\n",
    "    if len(cleaned) != topics_per_chunk:\n",
    "        return []\n",
    "\n",
    "    return cleaned\n"
   ],
   "id": "a3341f8af96ffd11",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:41.726173Z",
     "start_time": "2026-01-22T12:34:41.723199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _norm_label(label: str) -> str:\n",
    "    label = label.strip().lower()\n",
    "    label = re.sub(r\"\\s+\", \" \", label)\n",
    "    label = re.sub(r\"[^\\w\\s\\-&/]\", \"\", label)\n",
    "    return label.strip()"
   ],
   "id": "89fc729b0d9fe889",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:42.448949Z",
     "start_time": "2026-01-22T12:34:42.445618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_topics_frequency_based(\n",
    "    per_chunk_topics: List[List[Dict[str, Any]]],\n",
    "    final_k_topics: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    from collections import Counter, defaultdict\n",
    "\n",
    "    label_support = Counter()\n",
    "    label_canonical: Dict[str, str] = {}\n",
    "    kw_counts: Dict[str, Counter] = defaultdict(Counter)\n",
    "\n",
    "    for chunk_topics in per_chunk_topics:\n",
    "        for t in chunk_topics:\n",
    "            label = str(t.get(\"label\", \"\")).strip()\n",
    "            if not label:\n",
    "                continue\n",
    "            norm = _norm_label(label)\n",
    "            if not norm:\n",
    "                continue\n",
    "\n",
    "            label_support[norm] += 1\n",
    "            label_canonical.setdefault(norm, label)\n",
    "\n",
    "            for kw in t.get(\"keywords\", []) or []:\n",
    "                kw_s = str(kw).strip()\n",
    "                if kw_s:\n",
    "                    kw_counts[norm][kw_s] += 1\n",
    "\n",
    "    ranked_labels = sorted(label_support.items(), key=lambda x: (-x[1], x[0]))\n",
    "    selected = ranked_labels[: max(1, final_k_topics)]\n",
    "\n",
    "    merged: List[Dict[str, Any]] = []\n",
    "    for norm, support in selected:\n",
    "        top_keywords = [k for k, _ in kw_counts[norm].most_common(keywords_per_topic)]\n",
    "        merged.append(\n",
    "            {\n",
    "                \"label\": label_canonical.get(norm, norm),\n",
    "                \"keywords\": top_keywords,\n",
    "                \"support_chunks\": int(support),\n",
    "            }\n",
    "        )\n",
    "    return merged"
   ],
   "id": "5c7af8683142e562",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:43.278194Z",
     "start_time": "2026-01-22T12:34:43.275668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_prompt_space_factorisation(\n",
    "    *,\n",
    "    llm,\n",
    "    doc_chunks: List[Any],\n",
    "    variant: PromptVariant,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    final_k_topics: int,\n",
    ") -> Dict[str, Any]:\n",
    "    per_chunk_topics: List[List[Dict[str, Any]]] = []\n",
    "\n",
    "    for chunk in doc_chunks:\n",
    "        topics = extract_topics_for_chunk_factorised(\n",
    "            llm=llm,\n",
    "            variant=variant,\n",
    "            chunk=chunk,\n",
    "            topics_per_chunk=topics_per_chunk,\n",
    "            keywords_per_topic=keywords_per_topic,\n",
    "        )\n",
    "        per_chunk_topics.append(topics)\n",
    "\n",
    "    merged_topics = merge_topics_frequency_based(\n",
    "        per_chunk_topics=per_chunk_topics,\n",
    "        final_k_topics=final_k_topics,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"stage\": \"prompt_space_factorisation\",\n",
    "        \"variant_id\": variant.variant_id,\n",
    "        \"num_chunks\": len(doc_chunks),\n",
    "        \"topics_per_chunk\": topics_per_chunk,\n",
    "        \"keywords_per_topic\": keywords_per_topic,\n",
    "        \"final_k_topics\": final_k_topics,\n",
    "        \"topics\": merged_topics,\n",
    "    }"
   ],
   "id": "5631502cc2f04f8c",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:44.063154Z",
     "start_time": "2026-01-22T12:34:44.060802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_default_factorised_variant(\n",
    "    *,\n",
    "    include_exemplars: bool = True,\n",
    "    delimiter_open: str = \"<TEXT>\",\n",
    "    delimiter_close: str = \"</TEXT>\",\n",
    ") -> PromptVariant:\n",
    "    comps = PromptComponents(\n",
    "        role=default_role_financial_analyst(),\n",
    "        instruction=default_instruction(),\n",
    "        rubric=None,  # rendered at call-time with topics_per_chunk/keywords_per_topic\n",
    "        exemplars=(default_exemplars(input_text,output_json) if include_exemplars else []),\n",
    "        delimiter=Delimiter(open=delimiter_open, close=delimiter_close),\n",
    "    )\n",
    "    return PromptVariant(variant_id=\"factorised_default\", components=comps)\n"
   ],
   "id": "84f884f8186a5e24",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T14:17:52.164840Z",
     "start_time": "2026-01-21T14:17:51.801941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "variant = make_default_factorised_variant(include_exemplars=True)\n",
    "result = run_prompt_space_factorisation(\n",
    "     llm=llm,\n",
    "     doc_chunks=doc_chunk,\n",
    "     variant=variant,\n",
    "     topics_per_chunk=TOPICS_PER_CHUNK,\n",
    "     keywords_per_topic=KEYWORDS_PER_TOPIC,\n",
    "     final_k_topics=FINAL_K_TOPICS,\n",
    " )\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ],
   "id": "7d3b136de25d3794",
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m variant = make_default_factorised_variant(include_exemplars=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m result = \u001B[43mrun_prompt_space_factorisation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m     \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m     \u001B[49m\u001B[43mdoc_chunks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdoc_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m     \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m     \u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTOPICS_PER_CHUNK\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m     \u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m=\u001B[49m\u001B[43mKEYWORDS_PER_TOPIC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m     \u001B[49m\u001B[43mfinal_k_topics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mFINAL_K_TOPICS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[38;5;28mprint\u001B[39m(json.dumps(result, indent=\u001B[32m2\u001B[39m, ensure_ascii=\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mrun_prompt_space_factorisation\u001B[39m\u001B[34m(llm, doc_chunks, variant, topics_per_chunk, keywords_per_topic, final_k_topics)\u001B[39m\n\u001B[32m     10\u001B[39m per_chunk_topics: List[List[Dict[\u001B[38;5;28mstr\u001B[39m, Any]]] = []\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m doc_chunks:\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     topics = \u001B[43mextract_topics_for_chunk_factorised\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m     per_chunk_topics.append(topics)\n\u001B[32m     22\u001B[39m merged_topics = merge_topics_frequency_based(\n\u001B[32m     23\u001B[39m     per_chunk_topics=per_chunk_topics,\n\u001B[32m     24\u001B[39m     final_k_topics=final_k_topics,\n\u001B[32m     25\u001B[39m     keywords_per_topic=keywords_per_topic,\n\u001B[32m     26\u001B[39m )\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mextract_topics_for_chunk_factorised\u001B[39m\u001B[34m(llm, variant, chunk, topics_per_chunk, keywords_per_topic)\u001B[39m\n\u001B[32m     14\u001B[39m     variant = PromptVariant(\n\u001B[32m     15\u001B[39m         variant_id=variant.variant_id,\n\u001B[32m     16\u001B[39m         components=PromptComponents(\n\u001B[32m   (...)\u001B[39m\u001B[32m     22\u001B[39m         ),\n\u001B[32m     23\u001B[39m     )\n\u001B[32m     25\u001B[39m prompt = build_factorised_prompt(\n\u001B[32m     26\u001B[39m     variant=variant,\n\u001B[32m     27\u001B[39m     chunk_text=text,\n\u001B[32m     28\u001B[39m     topics_per_chunk=topics_per_chunk,\n\u001B[32m     29\u001B[39m     keywords_per_topic=keywords_per_topic,\n\u001B[32m     30\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m resp = \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m content = \u001B[38;5;28mgetattr\u001B[39m(resp, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(resp))\n\u001B[32m     34\u001B[39m data = _parse_json_loose(content)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    386\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    391\u001B[39m     **kwargs: Any,\n\u001B[32m    392\u001B[39m ) -> AIMessage:\n\u001B[32m    393\u001B[39m     config = ensure_config(config)\n\u001B[32m    394\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    395\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAIMessage\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    396\u001B[39m         cast(\n\u001B[32m    397\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m                \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    408\u001B[39m         ).message,\n\u001B[32m    409\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1117\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1108\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1110\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1114\u001B[39m     **kwargs: Any,\n\u001B[32m   1115\u001B[39m ) -> LLMResult:\n\u001B[32m   1116\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1117\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:927\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    925\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    926\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m927\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    933\u001B[39m         )\n\u001B[32m    934\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    935\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1221\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1219\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1220\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1221\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1222\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1225\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:593\u001B[39m, in \u001B[36mChatGroq._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    588\u001B[39m message_dicts, params = \u001B[38;5;28mself\u001B[39m._create_message_dicts(messages, stop)\n\u001B[32m    589\u001B[39m params = {\n\u001B[32m    590\u001B[39m     **params,\n\u001B[32m    591\u001B[39m     **kwargs,\n\u001B[32m    592\u001B[39m }\n\u001B[32m--> \u001B[39m\u001B[32m593\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmessage_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_chat_result(response, params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:464\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    244\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    245\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    246\u001B[39m     *,\n\u001B[32m   (...)\u001B[39m\u001B[32m    303\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m    304\u001B[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001B[32m    305\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    306\u001B[39m \u001B[33;03m    Creates a model response for the given chat conversation.\u001B[39;00m\n\u001B[32m    307\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    462\u001B[39m \u001B[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001B[39;00m\n\u001B[32m    463\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m464\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/openai/v1/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    470\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcitation_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcitation_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    471\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompound_custom\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompound_custom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdisable_tool_validation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_tool_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdocuments\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexclude_domains\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude_domains\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    475\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    478\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minclude_domains\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_domains\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minclude_reasoning\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_reasoning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msearch_settings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msearch_settings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    500\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    501\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    502\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    503\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    504\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    505\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    506\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    507\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    508\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    509\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    510\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    511\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    512\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/groq/_base_client.py:1242\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1228\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1229\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1230\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1237\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1238\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1239\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1240\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1241\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1242\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Prompt Engineering for GPT-based Topic Modeling /.venv/lib/python3.12/site-packages/groq/_base_client.py:1044\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1041\u001B[39m             err.response.read()\n\u001B[32m   1043\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1044\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1046\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1048\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages'}}"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T11:53:59.849504Z",
     "start_time": "2026-01-18T11:53:59.845034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save JSON\n",
    "out_path = Path(\"outputs\") / f\"META_prompt_space_factorisation_{FINAL_K_TOPICS}.json\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(result, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "print(f\"Saved: {out_path}\")"
   ],
   "id": "9a9a4dcc1a03db45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/META_prompt_space_factorisation_10.json\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility - News",
   "id": "480c348c79c1d953"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:49.790386Z",
     "start_time": "2026-01-22T12:34:49.402777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import platform\n",
    "import hashlib\n",
    "import importlib\n",
    "from datetime import datetime, timezone\n",
    "from importlib import metadata\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple"
   ],
   "id": "ed6e99371f49a102",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:50.201382Z",
     "start_time": "2026-01-22T12:34:50.199520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FEEDS = {\n",
    "    \"apple_newsroom\": \"https://www.apple.com/ca/newsroom/rss-feed.rss\",\n",
    "    \"macrumors\": \"https://feeds.macrumors.com/MacRumors-All\",\n",
    "    \"reddit_apple\": \"https://www.reddit.com/r/apple/.rss\",\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (topic-modeling; +contact-email@example.com)\"\n",
    "}"
   ],
   "id": "8d05ac1ed3fe5a9f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:51.326621Z",
     "start_time": "2026-01-22T12:34:51.325030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = 1000\n",
    "chunk_overlap = 100"
   ],
   "id": "833b0b8c1ab1bd8c",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:54.113382Z",
     "start_time": "2026-01-22T12:34:52.329026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows = []\n",
    "for source, feed_url in FEEDS.items():\n",
    "    d = feedparser.parse(feed_url)\n",
    "    for e in d.entries:\n",
    "        rows.append({\n",
    "            \"source\": source,\n",
    "            \"title\": getattr(e, \"title\", None),\n",
    "            \"link\": getattr(e, \"link\", None),\n",
    "            \"published\": getattr(e, \"published\", None),\n",
    "            \"summary\": getattr(e, \"summary\", None),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"link\"]).reset_index(drop=True)"
   ],
   "id": "5f7f266b194eb05b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:56.617051Z",
     "start_time": "2026-01-22T12:34:56.614762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "news_df = df.drop_duplicates(subset=[\"link\"]).reset_index(drop=True)\n",
    "news_df[\"article_id\"] = range(len(news_df))"
   ],
   "id": "8a626c16702472e1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:57.363978Z",
     "start_time": "2026-01-22T12:34:57.362017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_article_document(row) -> str:\n",
    "    title = (row.get(\"title\") or \"\").strip()\n",
    "    summary = (row.get(\"summary\") or \"\").strip()\n",
    "    return f\"{title}\\n\\n{summary}\".strip()"
   ],
   "id": "94938557fdc74a6e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:58.191721Z",
     "start_time": "2026-01-22T12:34:58.182786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "news_df[\"document_text\"] = news_df.apply(build_article_document, axis=1)\n",
    "news_df[[\"article_id\", \"source\", \"title\"]].head()"
   ],
   "id": "a42cd0b3e55730ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   article_id          source  \\\n",
       "0           0  apple_newsroom   \n",
       "1           1  apple_newsroom   \n",
       "2           2  apple_newsroom   \n",
       "3           3  apple_newsroom   \n",
       "4           4  apple_newsroom   \n",
       "\n",
       "                                               title  \n",
       "0  The new Apple Sainte-Catherine opens today in ...  \n",
       "1  Popular PC franchise Civilization comes to App...  \n",
       "2  Introducing Apple Creator Studio, an inspiring...  \n",
       "3  2025 marked a record-breaking year for Apple s...  \n",
       "4       Stay active in the new year with AppleWatch  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>apple_newsroom</td>\n",
       "      <td>The new Apple Sainte-Catherine opens today in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple_newsroom</td>\n",
       "      <td>Popular PC franchise Civilization comes to App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>apple_newsroom</td>\n",
       "      <td>Introducing Apple Creator Studio, an inspiring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>apple_newsroom</td>\n",
       "      <td>2025 marked a record-breaking year for Apple s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>apple_newsroom</td>\n",
       "      <td>Stay active in the new year with AppleWatch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:34:59.305248Z",
     "start_time": "2026-01-22T12:34:59.303568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Output folder\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "OUT_DIR = Path(\"outputs\") / \"news\" / f\"run_{RUN_ID}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "5f6e3ec291f209fb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:00.767618Z",
     "start_time": "2026-01-22T12:35:00.764930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _pkg_version(name: str) -> str:\n",
    "    try:\n",
    "        return metadata.version(name)\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "def write_run_metadata(*, out_dir: Path, run_cfg: Dict[str, Any]) -> Path:\n",
    "    meta = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"timestamp_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"packages\": {\n",
    "            \"pandas\": _pkg_version(\"pandas\"),\n",
    "            \"numpy\": _pkg_version(\"numpy\"),\n",
    "            \"langchain\": _pkg_version(\"langchain\"),\n",
    "            \"langchain-core\": _pkg_version(\"langchain-core\"),\n",
    "            \"langchain-groq\": _pkg_version(\"langchain-groq\"),\n",
    "            \"groq\": _pkg_version(\"groq\"),\n",
    "        },\n",
    "        \"config\": run_cfg,\n",
    "    }\n",
    "    p = out_dir / \"run_metadata.json\"\n",
    "    p.write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    return p"
   ],
   "id": "701abb6e82b54097",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:01.523899Z",
     "start_time": "2026-01-22T12:35:01.521774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _as_text(x: Any) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if hasattr(x, \"page_content\"):\n",
    "        return str(getattr(x, \"page_content\") or \"\")\n",
    "    return str(x)"
   ],
   "id": "209486d0bf7a1e43",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:02.269881Z",
     "start_time": "2026-01-22T12:35:02.267769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON parsing\n",
    "_JSON_OBJ_RE = re.compile(r\"\\{.*?\\}\", flags=re.DOTALL)\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"^\\s*```(?:json)?\\s*\", \"\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\s*```\\s*$\", \"\", s)\n",
    "    return s.strip()\n",
    "def _reject_constants(x: str) -> Any:\n",
    "    raise ValueError(f\"Invalid JSON constant: {x}\")\n"
   ],
   "id": "3b80ce8a2f30461d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:03.300275Z",
     "start_time": "2026-01-22T12:35:03.297707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_json_loose_to_dict(raw: Any) -> Dict[str, Any]:\n",
    "\n",
    "    if raw is None:\n",
    "        return {}\n",
    "\n",
    "    s = str(raw).strip()\n",
    "    if not s:\n",
    "        return {}\n",
    "\n",
    "    if s.lower() in {\"nan\", \"none\", \"null\"}:\n",
    "        return {}\n",
    "\n",
    "    s = _strip_code_fences(s)\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(s, parse_constant=_reject_constants)\n",
    "        return obj if isinstance(obj, dict) else {}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    m = _JSON_OBJ_RE.search(s)\n",
    "    if m:\n",
    "        try:\n",
    "            obj = json.loads(m.group(0), parse_constant=_reject_constants)\n",
    "            return obj if isinstance(obj, dict) else {}\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    return {}"
   ],
   "id": "32d424e62c8e66e2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:04.114414Z",
     "start_time": "2026-01-22T12:35:04.111896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate_topics_payload(data: Dict[str, Any], *, keywords_per_topic: int) -> List[Dict[str, Any]]:\n",
    "\n",
    "    topics = data.get(\"topics\", [])\n",
    "    if not isinstance(topics, list):\n",
    "        return []\n",
    "\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "    for t in topics:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "        label = str(t.get(\"label\", \"\")).strip()\n",
    "        kws = t.get(\"keywords\", [])\n",
    "        if not label or not isinstance(kws, list):\n",
    "            continue\n",
    "\n",
    "        keywords = [str(k).strip() for k in kws if str(k).strip()]\n",
    "        if len(keywords) != keywords_per_topic:\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\"label\": label, \"keywords\": keywords})\n",
    "\n",
    "    return cleaned"
   ],
   "id": "9052a22546918a90",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:04.927397Z",
     "start_time": "2026-01-22T12:35:04.924549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_jsonl(path: Path, record: Dict[str, Any]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "PER_CHUNK_LOG = OUT_DIR / \"per_chunk_outputs.jsonl\"\n",
    "PARSE_FAIL_LOG = OUT_DIR / \"parse_failures.jsonl\"\n",
    "ARTIFACTS_DIR = OUT_DIR / \"artifacts\"\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "f660b4d76d084570",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:05.732301Z",
     "start_time": "2026-01-22T12:35:05.730005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_article_chunks(article_id: int, chunks: List[Any]) -> Path:\n",
    "    chunk_texts = [_as_text(c) for c in chunks]\n",
    "    rec = {\n",
    "        \"article_id\": int(article_id),\n",
    "        \"num_chunks\": len(chunk_texts),\n",
    "        \"chunks_sha256\": [sha256_text(t) for t in chunk_texts],\n",
    "        \"chunks\": chunk_texts,\n",
    "    }\n",
    "    p = ARTIFACTS_DIR / f\"article_{int(article_id)}_chunks.json\"\n",
    "    p.write_text(json.dumps(rec, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return p"
   ],
   "id": "f6452ba1a37a095a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:06.723162Z",
     "start_time": "2026-01-22T12:35:06.719142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_topics_for_chunk_factorised_logged(\n",
    "    *,\n",
    "    llm,\n",
    "    variant,\n",
    "    chunk: Any,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    article_id: Optional[int] = None,\n",
    "    chunk_idx: Optional[int] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    text = _as_text(chunk)\n",
    "\n",
    "    c = variant.components\n",
    "    rubric = c.rubric\n",
    "    if rubric is None:\n",
    "        rubric = default_rubric(topics_per_chunk, keywords_per_topic)\n",
    "        variant = PromptVariant(\n",
    "            variant_id=variant.variant_id,\n",
    "            components=PromptComponents(\n",
    "                role=c.role,\n",
    "                instruction=c.instruction,\n",
    "                rubric=rubric,\n",
    "                exemplars=c.exemplars,\n",
    "                delimiter=c.delimiter,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    prompt = build_factorised_prompt(\n",
    "        variant=variant,\n",
    "        chunk_text=text,\n",
    "        topics_per_chunk=topics_per_chunk,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    resp = llm.invoke(prompt)\n",
    "    raw = getattr(resp, \"content\", str(resp))\n",
    "\n",
    "    data = parse_json_loose_to_dict(raw)\n",
    "    topics = validate_topics_payload(data, keywords_per_topic=keywords_per_topic)\n",
    "    parsed_ok = bool(topics)\n",
    "\n",
    "    log_jsonl(\n",
    "        PER_CHUNK_LOG,\n",
    "        {\n",
    "            \"article_id\": int(article_id) if article_id is not None else None,\n",
    "            \"variant_id\": getattr(variant, \"variant_id\", None),\n",
    "            \"chunk_idx\": int(chunk_idx) if chunk_idx is not None else None,\n",
    "            \"chunk_sha256\": sha256_text(text),\n",
    "            \"raw_preview\": str(raw)[:2000],\n",
    "            \"parsed_ok\": parsed_ok,\n",
    "            \"num_topics\": len(topics),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if not parsed_ok:\n",
    "        log_jsonl(\n",
    "            PARSE_FAIL_LOG,\n",
    "            {\n",
    "                \"article_id\": int(article_id) if article_id is not None else None,\n",
    "                \"variant_id\": getattr(variant, \"variant_id\", None),\n",
    "                \"chunk_idx\": int(chunk_idx) if chunk_idx is not None else None,\n",
    "                \"chunk_sha256\": sha256_text(text),\n",
    "                \"raw\": str(raw)[:8000],  # bounded, but keeps enough for debugging\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return topics\n"
   ],
   "id": "e66fa7d69bd0c6c",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:07.715675Z",
     "start_time": "2026-01-22T12:35:07.712786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_prompt_space_factorisation_logged(\n",
    "    *,\n",
    "    llm,\n",
    "    doc_chunks: List[Any],\n",
    "    variant,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    final_k_topics: int,\n",
    "    article_id: Optional[int] = None,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    per_chunk_topics: List[List[Dict[str, Any]]] = []\n",
    "    for i, chunk in enumerate(doc_chunks):\n",
    "        topics = extract_topics_for_chunk_factorised_logged(\n",
    "            llm=llm,\n",
    "            variant=variant,\n",
    "            chunk=chunk,\n",
    "            topics_per_chunk=topics_per_chunk,\n",
    "            keywords_per_topic=keywords_per_topic,\n",
    "            article_id=article_id,\n",
    "            chunk_idx=i,\n",
    "        )\n",
    "        per_chunk_topics.append(topics)\n",
    "\n",
    "    merged_topics = merge_topics_frequency_based(\n",
    "        per_chunk_topics=per_chunk_topics,\n",
    "        final_k_topics=final_k_topics,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"stage\": \"prompt_space_factorisation\",\n",
    "        \"variant_id\": variant.variant_id,\n",
    "        \"num_chunks\": len(doc_chunks),\n",
    "        \"topics_per_chunk\": topics_per_chunk,\n",
    "        \"keywords_per_topic\": keywords_per_topic,\n",
    "        \"final_k_topics\": final_k_topics,\n",
    "        \"topics\": merged_topics,\n",
    "    }\n"
   ],
   "id": "e78935a5c1dac2ad",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:08.993169Z",
     "start_time": "2026-01-22T12:35:08.991354Z"
    }
   },
   "cell_type": "code",
   "source": "variant = make_default_factorised_variant(include_exemplars=True)",
   "id": "4e3cc9897dcffe66",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:09.983508Z",
     "start_time": "2026-01-22T12:35:09.973322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_cfg = {\n",
    "    \"variant_id\": getattr(variant, \"variant_id\", None),\n",
    "    \"topics_per_chunk\": TOPICS_PER_CHUNK,\n",
    "    \"keywords_per_topic\": KEYWORDS_PER_TOPIC,\n",
    "    \"final_k_topics\": FINAL_K_TOPICS,\n",
    "     \"chunk_size\": chunk_size,\n",
    "     \"chunk_overlap\": chunk_overlap,\n",
    "     \"model\": model,\n",
    "}\n",
    "write_run_metadata(out_dir=OUT_DIR, run_cfg=run_cfg)\n",
    "\n",
    "print(f\"Reproducibility logging enabled. Outputs will be written to: {OUT_DIR}\")"
   ],
   "id": "6f69a84d65dd1c67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility logging enabled. Outputs will be written to: outputs/news/run_20260122T123459Z\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:11.195436Z",
     "start_time": "2026-01-22T12:35:11.193290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_one_article(row) -> list[Document]:\n",
    "    doc = Document(\n",
    "        page_content=row[\"document_text\"],\n",
    "        metadata={\n",
    "            \"article_id\": int(row[\"article_id\"]),\n",
    "            \"source\": row.get(\"source\"),\n",
    "            \"title\": row.get(\"title\"),\n",
    "            \"link\": row.get(\"link\"),\n",
    "            \"published\": row.get(\"published\"),\n",
    "        },\n",
    "    )\n",
    "    return text_splitter.split_documents([doc])"
   ],
   "id": "8f62650c94168589",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:15.492581Z",
     "start_time": "2026-01-22T12:35:15.490599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"model:\", llm._default_params.get(\"model\"))\n",
    "print(\"max_tokens:\", llm._default_params.get(\"max_tokens\"))"
   ],
   "id": "cd0faa551e5c71fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: meta-llama/llama-4-maverick-17b-128e-instruct\n",
      "max_tokens: None\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:35:27.892538Z",
     "start_time": "2026-01-22T12:35:27.890617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")"
   ],
   "id": "d195a769215f8a81",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T12:58:06.648601Z",
     "start_time": "2026-01-22T12:35:42.250874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "for _, row in tqdm(news_df.iterrows(), total=len(news_df)):\n",
    "    article_id = int(row[\"article_id\"])\n",
    "\n",
    "    doc_chunks = chunk_one_article(row)\n",
    "    if not doc_chunks:\n",
    "        continue\n",
    "\n",
    "    save_article_chunks(article_id, doc_chunks)\n",
    "\n",
    "    out = run_prompt_space_factorisation_logged(\n",
    "        llm=llm,\n",
    "        doc_chunks=doc_chunks,\n",
    "        variant=variant,\n",
    "        topics_per_chunk=TOPICS_PER_CHUNK,\n",
    "        keywords_per_topic=KEYWORDS_PER_TOPIC,\n",
    "        final_k_topics=FINAL_K_TOPICS,\n",
    "        article_id=article_id,\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"article_id\": article_id,\n",
    "        \"source\": row.get(\"source\"),\n",
    "        \"title\": row.get(\"title\"),\n",
    "        \"link\": row.get(\"link\"),\n",
    "        \"published\": row.get(\"published\"),\n",
    "        \"topics_json\": out,\n",
    "    })\n",
    "\n",
    "len(results), results[0][\"topics_json\"].keys() if results else (0, [])"
   ],
   "id": "42b41af33c427b8b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 65/65 [22:24<00:00, 20.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65,\n",
       " dict_keys(['stage', 'variant_id', 'num_chunks', 'topics_per_chunk', 'keywords_per_topic', 'final_k_topics', 'topics']))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T13:49:58.568116Z",
     "start_time": "2026-01-22T13:49:58.565963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for t in results[0][\"topics_json\"][\"topics\"]:\n",
    "    print(t)"
   ],
   "id": "938790198126f23f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Brand Presence', 'keywords': ['Apple', 'brand', 'identity', 'image', 'awareness', 'recognition', 'loyalty', 'reputation'], 'support_chunks': 1}\n",
      "{'label': 'Business Development', 'keywords': ['development', 'growth', 'expansion', 'opportunities', 'investment', 'initiative', 'strategy', 'prospects'], 'support_chunks': 1}\n",
      "{'label': 'Customer Engagement', 'keywords': ['customers', 'engagement', 'interaction', 'service', 'support', 'community', 'outreach', 'events'], 'support_chunks': 1}\n",
      "{'label': 'Geographic Expansion', 'keywords': ['Montreal', 'Canada', 'region', 'market', 'expansion', 'presence', 'location', 'site'], 'support_chunks': 1}\n",
      "{'label': 'Marketing Strategy', 'keywords': ['marketing', 'strategy', 'promotion', 'advertising', 'publicity', 'launch', 'campaign', 'buzz'], 'support_chunks': 1}\n",
      "{'label': 'Retail Experience', 'keywords': ['reimagined', 'space', 'customers', 'experience', 'shopping', 'environment', 'design', 'layout'], 'support_chunks': 1}\n",
      "{'label': 'Store Opening', 'keywords': ['Apple', 'Sainte-Catherine', 'store', 'opening', 'Montreal', 'new', 'location', 'doors'], 'support_chunks': 1}\n",
      "{'label': 'Store Operations', 'keywords': ['store', 'operations', 'management', 'staffing', 'training', 'inventory', 'logistics', 'supply'], 'support_chunks': 1}\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T13:51:17.934530Z",
     "start_time": "2026-01-22T13:51:17.931312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_dir = OUT_DIR / \"final\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(final_dir / f\"article_{article_id}_final.json\").write_text(\n",
    "    json.dumps(out, indent=2, ensure_ascii=False),\n",
    "    encoding=\"utf-8\",\n",
    ")"
   ],
   "id": "82da9b5d11787194",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2284"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b4a028018885f770"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
