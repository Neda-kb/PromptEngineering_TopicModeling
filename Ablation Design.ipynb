{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:38.439826Z",
     "start_time": "2026-01-11T14:07:37.716740Z"
    }
   },
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import Load_Data\n",
    "import LLM_setting\n",
    "import Chunking\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import Data_Processing as dp\n",
    "from Data_Processing import merge_topics_frequency_based as m\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import Counter, defaultdict, deque\n",
    "from typing import List, Optional, Dict, Any , Sequence, Union\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from difflib import SequenceMatcher\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import BSHTMLLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:39.198056Z",
     "start_time": "2026-01-11T14:07:39.194501Z"
    }
   },
   "cell_type": "code",
   "source": "_ = load_dotenv(find_dotenv())",
   "id": "aa0826b0fb1dd5a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:40.083076Z",
     "start_time": "2026-01-11T14:07:40.018656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#LLM Setting\n",
    "model = \"openai/gpt-oss-120b\"\n",
    "temperature = 0\n",
    "llm_setting = LLM_setting.LLMSetting(MODEL=model, TEMPERATURE=temperature)\n",
    "llm = LLM_setting.setting(llm_setting)"
   ],
   "id": "a69d9035bdef5f58",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:41.921273Z",
     "start_time": "2026-01-11T14:07:40.871564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read Data\n",
    "file_path = Path(\"data/raw_filings/320193_000032019325000079_aapl-20250927.htm\")\n",
    "cfg = Load_Data.LoadData(file_path=file_path)\n",
    "docs = Load_Data.load_data(cfg)"
   ],
   "id": "e7bc29f3318214af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:43.273365Z",
     "start_time": "2026-01-11T14:07:43.182148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chunking\n",
    "chunk_size= 2500\n",
    "chunk_overlap = 250\n",
    "data_processing=Chunking.DataProcessing(file_path,chunk_size,chunk_overlap)\n",
    "doc_chunk =Chunking.chunks(data_processing)"
   ],
   "id": "75d9597779c3ec3c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:44.025012Z",
     "start_time": "2026-01-11T14:07:44.022671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TOPICS_PER_CHUNK = 8\n",
    "KEYWORDS_PER_TOPIC = 8\n",
    "FINAL_K_TOPICS = 10"
   ],
   "id": "abf585c4520a5151",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:44.659473Z",
     "start_time": "2026-01-11T14:07:44.655590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = (\n",
    "    \"We experienced strong net sales growth driven by iPhone and Services. \"\n",
    "    \"Supply constraints eased. Foreign exchange impacted revenue. \"\n",
    "    \"We repurchased shares and increased the dividend.\"\n",
    ")\n",
    "output_json = {\n",
    "    \"topics\": [\n",
    "        {\"label\": \"Product Revenue Drivers\",\n",
    "         \"keywords\": [\"iPhone\", \"Services\", \"net sales\", \"growth\", \"revenue\", \"demand\", \"mix\", \"products\"]},\n",
    "        {\"label\": \"Supply Chain Conditions\",\n",
    "         \"keywords\": [\"supply\", \"constraints\", \"inventory\", \"availability\", \"production\", \"lead times\", \"logistics\",\n",
    "                      \"capacity\"]},\n",
    "        {\"label\": \"Foreign Exchange Effects\",\n",
    "         \"keywords\": [\"foreign exchange\", \"currency\", \"FX\", \"headwind\", \"translation\", \"rates\", \"impact\", \"revenue\"]},\n",
    "        {\"label\": \"Capital Return Program\",\n",
    "         \"keywords\": [\"share repurchase\", \"buyback\", \"dividend\", \"capital return\", \"shareholders\", \"authorization\",\n",
    "                      \"cash\", \"stock\"]},\n",
    "        {\"label\": \"Operating Performance\",\n",
    "         \"keywords\": [\"margin\", \"profitability\", \"operating income\", \"expenses\", \"costs\", \"efficiency\", \"performance\",\n",
    "                      \"results\"]},\n",
    "        {\"label\": \"Market Conditions\",\n",
    "         \"keywords\": [\"macroeconomic\", \"consumer\", \"competition\", \"pricing\", \"market\", \"uncertainty\", \"trends\",\n",
    "                      \"demand\"]},\n",
    "        {\"label\": \"Guidance and Outlook\",\n",
    "         \"keywords\": [\"outlook\", \"expectations\", \"guidance\", \"future\", \"forecast\", \"assumptions\", \"trend\", \"risks\"]},\n",
    "        {\"label\": \"Shareholder Value\",\n",
    "         \"keywords\": [\"value\", \"returns\", \"EPS\", \"share count\", \"capital allocation\", \"liquidity\", \"cash flow\",\n",
    "                      \"investment\"]},\n",
    "    ]\n",
    "}"
   ],
   "id": "82d5895202e9420c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:45.233455Z",
     "start_time": "2026-01-11T14:07:45.231562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = Path(\"outputs/ablation_runs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "35c581e65965ba69",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:45.736508Z",
     "start_time": "2026-01-11T14:07:45.734423Z"
    }
   },
   "cell_type": "code",
   "source": "_JSON_OBJ_RE = re.compile(r\"\\{.*\\}\", flags=re.DOTALL)",
   "id": "abc35854b56711c1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:46.258939Z",
     "start_time": "2026-01-11T14:07:46.256412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Exemplar:\n",
    "    input_text: str\n",
    "    output_json: Dict[str, Any]"
   ],
   "id": "f5dc7ea2a4ed5994",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:46.757264Z",
     "start_time": "2026-01-11T14:07:46.754779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Delimiter:\n",
    "    open: str = \"<TEXT>\"\n",
    "    close: str = \"</TEXT>\""
   ],
   "id": "b126e249836312c1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:47.255937Z",
     "start_time": "2026-01-11T14:07:47.253234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class PromptComponents:\n",
    "    role: Optional[str]\n",
    "    instruction: str\n",
    "    rubric: Optional[str]\n",
    "    exemplars: Sequence[Exemplar]\n",
    "    delimiter: Delimiter"
   ],
   "id": "1d48a2c2cce719ba",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:47.809672Z",
     "start_time": "2026-01-11T14:07:47.804190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class PromptVariant:\n",
    "    variant_id: str\n",
    "    components: PromptComponents\n",
    "def default_instruction() -> str:\n",
    "    return \"Identify and label the main topics in this corpus.\"\n",
    "\n",
    "def default_role_financial_analyst() -> str:\n",
    "    return \"You are a financial analyst.\"\n",
    "\n",
    "def default_rubric(topics_per_chunk: int, keywords_per_topic: int) -> str:\n",
    "\n",
    "    schema_line = 'Schema: {\"topics\":[{\"label\":str,\"keywords\":[str,...]}]}'\n",
    "    return (\n",
    "        \"Rubric:\\n\"\n",
    "        \"- Topics should be coherent, specific, and meaningful for financial reporting.\\n\"\n",
    "        \"- Topic labels should be short noun phrases (avoid generic labels like 'Other').\\n\"\n",
    "        \"- Topics should be mutually distinct (minimise keyword overlap).\\n\"\n",
    "        \"- Prefer substantive business/financial themes over boilerplate artifacts.\\n\"\n",
    "        \"- Avoid pure dates, identifiers, XBRL-like tags, or URL-like tokens as keywords.\\n\\n\"\n",
    "        \"Output constraints:\\n\"\n",
    "        \"Use ONLY the text inside the delimiter.\\n\"\n",
    "        f\"Return exactly {topics_per_chunk} topics.\\n\"\n",
    "        f\"Each topic must have exactly {keywords_per_topic} keywords.\\n\"\n",
    "        \"Output MUST be valid JSON only (no Markdown, no extra text).\\n\"\n",
    "        f\"{schema_line}\\n\"\n",
    "    )\n",
    "\n",
    "def default_exemplars(input_text:Any,output_json:Dict) -> List[Exemplar]:\n",
    "    return [\n",
    "        Exemplar(\n",
    "            input_text,\n",
    "            output_json,\n",
    "        )\n",
    "    ]\n",
    "def build_factorised_prompt(\n",
    "    variant: PromptVariant,\n",
    "    chunk_text: str,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> str:\n",
    "    c = variant.components\n",
    "\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # 1) role\n",
    "    if c.role:\n",
    "        parts.append(c.role.strip())\n",
    "\n",
    "    # 2) instruction\n",
    "    parts.append(c.instruction.strip())\n",
    "\n",
    "    # 3) rubric\n",
    "    if c.rubric:\n",
    "        parts.append(c.rubric.strip())\n",
    "\n",
    "    # 4) exemplars (optional)\n",
    "    if c.exemplars:\n",
    "        ex_lines: List[str] = [\"Exemplars:\"]\n",
    "        for i, ex in enumerate(c.exemplars, start=1):\n",
    "            ex_out = json.dumps(ex.output_json, ensure_ascii=False)\n",
    "            ex_lines.append(f\"[Example {i}]\")\n",
    "            ex_lines.append(\"Input:\")\n",
    "            ex_lines.append(ex.input_text.strip())\n",
    "            ex_lines.append(\"Output (JSON):\")\n",
    "            ex_lines.append(ex_out)\n",
    "            ex_lines.append(\"\")  # spacer\n",
    "        parts.append(\"\\n\".join(ex_lines).strip())\n",
    "\n",
    "    # 5) delimiter\n",
    "    d = c.delimiter\n",
    "    parts.append(f\"{d.open}\\n{chunk_text}\\n{d.close}\")\n",
    "\n",
    "    return \"\\n\\n\".join([p for p in parts if p and p.strip()])"
   ],
   "id": "4e9d8800e46a9cf4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:48.586217Z",
     "start_time": "2026-01-11T14:07:48.583733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _as_text(obj: Any) -> str:\n",
    "    if hasattr(obj, \"page_content\"):\n",
    "        return obj.page_content\n",
    "    if hasattr(obj, \"chunk\"):\n",
    "        ch = getattr(obj, \"chunk\", None)\n",
    "        if hasattr(ch, \"page_content\"):\n",
    "            return ch.page_content\n",
    "        return str(ch)\n",
    "    return str(obj)"
   ],
   "id": "727d8ab4e73999d8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:49.272510Z",
     "start_time": "2026-01-11T14:07:49.269733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _parse_json_loose(s: str) -> Dict[str, Any]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = _JSON_OBJ_RE.search(s)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}"
   ],
   "id": "6db944b519990ab0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:50.145846Z",
     "start_time": "2026-01-11T14:07:50.142788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_topics_for_chunk_factorised(\n",
    "    llm,\n",
    "    variant: PromptVariant,\n",
    "    chunk: Any,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    text = _as_text(chunk)\n",
    "\n",
    "    c = variant.components\n",
    "    rubric = c.rubric\n",
    "    if rubric is None:\n",
    "        rubric = default_rubric(topics_per_chunk, keywords_per_topic)\n",
    "        variant = PromptVariant(\n",
    "            variant_id=variant.variant_id,\n",
    "            components=PromptComponents(\n",
    "                role=c.role,\n",
    "                instruction=c.instruction,\n",
    "                rubric=rubric,\n",
    "                exemplars=c.exemplars,\n",
    "                delimiter=c.delimiter,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    prompt = build_factorised_prompt(\n",
    "        variant=variant,\n",
    "        chunk_text=text,\n",
    "        topics_per_chunk=topics_per_chunk,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    TOKEN_TRACKER.throttle_if_needed()\n",
    "    if MIN_SLEEP_SECONDS:\n",
    "        time.sleep(MIN_SLEEP_SECONDS)\n",
    "\n",
    "    resp = invoke_with_backoff(llm, prompt)\n",
    "    content = getattr(resp, \"content\", str(resp))\n",
    "\n",
    "    _ = TOKEN_TRACKER.record(prompt=prompt, completion=content, resp=resp)\n",
    "\n",
    "    data = _parse_json_loose(content)\n",
    "\n",
    "    topics = data.get(\"topics\", [])\n",
    "    if not isinstance(topics, list):\n",
    "        return []\n",
    "\n",
    "    cleaned: List[Dict[str, Any]] = []\n",
    "    for t in topics:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "        label = str(t.get(\"label\", \"\")).strip()\n",
    "        kws = t.get(\"keywords\", [])\n",
    "        if not label or not isinstance(kws, list):\n",
    "            continue\n",
    "        keywords = [str(k).strip() for k in kws if str(k).strip()]\n",
    "\n",
    "        if len(keywords) != keywords_per_topic:\n",
    "            continue\n",
    "        cleaned.append({\"label\": label, \"keywords\": keywords})\n",
    "\n",
    "    if len(cleaned) != topics_per_chunk:\n",
    "        return []\n",
    "\n",
    "    return cleaned\n"
   ],
   "id": "cc28709dc1ee189c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:50.926675Z",
     "start_time": "2026-01-11T14:07:50.924870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _norm_label(label: str) -> str:\n",
    "    label = label.strip().lower()\n",
    "    label = re.sub(r\"\\s+\", \" \", label)\n",
    "    label = re.sub(r\"[^\\w\\s\\-&/]\", \"\", label)\n",
    "    return label.strip()"
   ],
   "id": "b110fefd429279d8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:51.799985Z",
     "start_time": "2026-01-11T14:07:51.795592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_topics_frequency_based(\n",
    "    per_chunk_topics: List[List[Dict[str, Any]]],\n",
    "    final_k_topics: int,\n",
    "    keywords_per_topic: int,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    from collections import Counter, defaultdict\n",
    "\n",
    "    label_support = Counter()\n",
    "    label_canonical: Dict[str, str] = {}\n",
    "    kw_counts: Dict[str, Counter] = defaultdict(Counter)\n",
    "\n",
    "    for chunk_topics in per_chunk_topics:\n",
    "        for t in chunk_topics:\n",
    "            label = str(t.get(\"label\", \"\")).strip()\n",
    "            if not label:\n",
    "                continue\n",
    "            norm = _norm_label(label)\n",
    "            if not norm:\n",
    "                continue\n",
    "\n",
    "            label_support[norm] += 1\n",
    "            label_canonical.setdefault(norm, label)\n",
    "\n",
    "            for kw in t.get(\"keywords\", []) or []:\n",
    "                kw_s = str(kw).strip()\n",
    "                if kw_s:\n",
    "                    kw_counts[norm][kw_s] += 1\n",
    "\n",
    "    ranked_labels = sorted(label_support.items(), key=lambda x: (-x[1], x[0]))\n",
    "    selected = ranked_labels[: max(1, final_k_topics)]\n",
    "\n",
    "    merged: List[Dict[str, Any]] = []\n",
    "    for norm, support in selected:\n",
    "        top_keywords = [k for k, _ in kw_counts[norm].most_common(keywords_per_topic)]\n",
    "        merged.append(\n",
    "            {\n",
    "                \"label\": label_canonical.get(norm, norm),\n",
    "                \"keywords\": top_keywords,\n",
    "                \"support_chunks\": int(support),\n",
    "            }\n",
    "        )\n",
    "    return merged"
   ],
   "id": "ab84913c4c9a9a29",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:53.367972Z",
     "start_time": "2026-01-11T14:07:53.364259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_prompt_space_factorisation(\n",
    "    *,\n",
    "    llm,\n",
    "    doc_chunks: List[Any],\n",
    "    variant: PromptVariant,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    final_k_topics: int,\n",
    ") -> Dict[str, Any]:\n",
    "    per_chunk_topics: List[List[Dict[str, Any]]] = []\n",
    "\n",
    "    for chunk in doc_chunks:\n",
    "        topics = extract_topics_for_chunk_factorised(\n",
    "            llm=llm,\n",
    "            variant=variant,\n",
    "            chunk=chunk,\n",
    "            topics_per_chunk=topics_per_chunk,\n",
    "            keywords_per_topic=keywords_per_topic,\n",
    "        )\n",
    "        per_chunk_topics.append(topics)\n",
    "\n",
    "    merged_topics = merge_topics_frequency_based(\n",
    "        per_chunk_topics=per_chunk_topics,\n",
    "        final_k_topics=final_k_topics,\n",
    "        keywords_per_topic=keywords_per_topic,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"stage\": \"prompt_space_factorisation\",\n",
    "        \"variant_id\": variant.variant_id,\n",
    "        \"num_chunks\": len(doc_chunks),\n",
    "        \"topics_per_chunk\": topics_per_chunk,\n",
    "        \"keywords_per_topic\": keywords_per_topic,\n",
    "        \"final_k_topics\": final_k_topics,\n",
    "        \"topics\": merged_topics,\n",
    "    }"
   ],
   "id": "82b828fdd3f09f6e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:54.912519Z",
     "start_time": "2026-01-11T14:07:54.909733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_default_factorised_variant(\n",
    "    *,\n",
    "    include_exemplars: bool = True,\n",
    "    delimiter_open: str = \"<TEXT>\",\n",
    "    delimiter_close: str = \"</TEXT>\",\n",
    ") -> PromptVariant:\n",
    "    comps = PromptComponents(\n",
    "        role=default_role_financial_analyst(),\n",
    "        instruction=default_instruction(),\n",
    "        rubric=None,\n",
    "        exemplars=(default_exemplars(input_text,output_json) if include_exemplars else []),\n",
    "        delimiter=Delimiter(open=delimiter_open, close=delimiter_close),\n",
    "    )\n",
    "    return PromptVariant(variant_id=\"factorised_default\", components=comps)"
   ],
   "id": "451bea52a0a086a3",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:56.008683Z",
     "start_time": "2026-01-11T14:07:56.004961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_ablation_variants(\n",
    "    base_variant: PromptVariant,\n",
    "    *,\n",
    "    include_role: bool = True,\n",
    "    include_rubric: bool = True,\n",
    "    include_exemplars: bool = True,\n",
    "    include_delimiter: bool = True,\n",
    ") -> PromptVariant:\n",
    "    c = base_variant.components\n",
    "\n",
    "    role = c.role if include_role else None\n",
    "    rubric = (c.rubric if c.rubric is not None else None) if include_rubric else \"\"\n",
    "    exemplars = c.exemplars if include_exemplars else []\n",
    "    delimiter = c.delimiter if include_delimiter else Delimiter(open=\"\", close=\"\")\n",
    "\n",
    "    comps = PromptComponents(\n",
    "        role=role,\n",
    "        instruction=c.instruction,\n",
    "        rubric=rubric,\n",
    "        exemplars=exemplars,\n",
    "        delimiter=delimiter,\n",
    "    )\n",
    "\n",
    "    off = []\n",
    "    if not include_role: off.append(\"role\")\n",
    "    if not include_rubric: off.append(\"rubric\")\n",
    "    if not include_exemplars: off.append(\"exemplars\")\n",
    "    if not include_delimiter: off.append(\"delimiter\")\n",
    "    variant_id = \"abl_full\" if not off else \"abl_minus_\" + \"_\".join(off)\n",
    "    return PromptVariant(variant_id=variant_id, components=comps)\n",
    "\n",
    "\n",
    "def generate_ablation_variants_from_base(base_variant: PromptVariant) -> List[PromptVariant]:\n",
    "\n",
    "    variants: List[PromptVariant] = []\n",
    "    for mask in range(16):\n",
    "        variants.append(\n",
    "            make_ablation_variants(\n",
    "                base_variant,\n",
    "                include_role=bool(mask & 1),\n",
    "                include_rubric=bool(mask & 2),\n",
    "                include_exemplars=bool(mask & 4),\n",
    "                include_delimiter=bool(mask & 8),\n",
    "            )\n",
    "        )\n",
    "    return variants\n"
   ],
   "id": "a19fd70836803658",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:56.880142Z",
     "start_time": "2026-01-11T14:07:56.872837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_ablation_extraction_only(\n",
    "    *,\n",
    "    llm,\n",
    "    doc_chunks: List[Any],\n",
    "    base_variant: PromptVariant,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    final_k_topics: int,\n",
    "    output_dir: Path,\n",
    ") -> pd.DataFrame:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    variants = generate_ablation_variants_from_base(base_variant)\n",
    "    rows = []\n",
    "\n",
    "    for v in variants:\n",
    "        per_chunk_topics: List[List[Dict[str, Any]]] = []\n",
    "        parse_failures = 0\n",
    "\n",
    "        for chunk in doc_chunks:\n",
    "            topics = extract_topics_for_chunk_factorised(\n",
    "                llm=llm,\n",
    "                variant=v,\n",
    "                chunk=chunk,\n",
    "                topics_per_chunk=topics_per_chunk,\n",
    "                keywords_per_topic=keywords_per_topic,\n",
    "            )\n",
    "            if not topics:\n",
    "                parse_failures += 1\n",
    "            per_chunk_topics.append(topics)\n",
    "\n",
    "        merged = merge_topics_frequency_based(\n",
    "            per_chunk_topics=per_chunk_topics,\n",
    "            final_k_topics=final_k_topics,\n",
    "            keywords_per_topic=keywords_per_topic,\n",
    "        )\n",
    "\n",
    "        run = {\n",
    "            \"stage\": \"ablation_extraction_only\",\n",
    "            \"variant_id\": v.variant_id,\n",
    "            \"num_chunks\": len(doc_chunks),\n",
    "            \"topics_per_chunk\": topics_per_chunk,\n",
    "            \"keywords_per_topic\": keywords_per_topic,\n",
    "            \"final_k_topics\": final_k_topics,\n",
    "            \"parse_failures\": int(parse_failures),\n",
    "            \"topics\": merged,\n",
    "            \"per_chunk_topics\": per_chunk_topics,\n",
    "        }\n",
    "\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        out_path = output_dir / f\"{ts}_{v.variant_id}.json\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(run, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        rows.append({\n",
    "            \"variant_id\": v.variant_id,\n",
    "            \"parse_failures\": int(parse_failures),\n",
    "            \"out_file\": str(out_path),\n",
    "        })\n",
    "\n",
    "    manifest = pd.DataFrame(rows).sort_values([\"parse_failures\", \"variant_id\"], ascending=[True, True])\n",
    "    manifest.to_csv(output_dir / \"manifest.csv\", index=False)\n",
    "    return manifest\n",
    "\n",
    "base_variant = make_default_factorised_variant(include_exemplars=True)\n",
    "variants = generate_ablation_variants_from_base(base_variant)\n",
    "variants[:3]\n"
   ],
   "id": "515a7cf15a93326a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PromptVariant(variant_id='abl_minus_role_rubric_exemplars_delimiter', components=PromptComponents(role=None, instruction='Identify and label the main topics in this corpus.', rubric='', exemplars=[], delimiter=Delimiter(open='', close=''))),\n",
       " PromptVariant(variant_id='abl_minus_rubric_exemplars_delimiter', components=PromptComponents(role='You are a financial analyst.', instruction='Identify and label the main topics in this corpus.', rubric='', exemplars=[], delimiter=Delimiter(open='', close=''))),\n",
       " PromptVariant(variant_id='abl_minus_role_exemplars_delimiter', components=PromptComponents(role=None, instruction='Identify and label the main topics in this corpus.', rubric=None, exemplars=[], delimiter=Delimiter(open='', close='')))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:58.062469Z",
     "start_time": "2026-01-11T14:07:58.054026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def utc_stamp():\n",
    "\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def done_file(output_dir: Path) -> Path:\n",
    "    return output_dir / \"done_variants.json\"\n",
    "\n",
    "def load_done(output_dir: Path) -> set:\n",
    "    done_path = output_dir / \"done_variants.json\"\n",
    "    if done_path.exists():\n",
    "        return set(json.loads(done_path.read_text(encoding=\"utf-8\")))\n",
    "    return set()\n",
    "\n",
    "def save_done(output_dir: Path, done_set: set):\n",
    "    done_path = output_dir / \"done_variants.json\"\n",
    "    done_path.write_text(json.dumps(sorted(done_set), indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "TPM_LIMIT = int(os.getenv(\"TPM_LIMIT\", \"0\"))\n",
    "TPM_SAFETY = float(os.getenv(\"TPM_SAFETY\", \"0.90\"))\n",
    "MIN_SLEEP_SECONDS = float(os.getenv(\"MIN_SLEEP_SECONDS\", \"2\"))\n",
    "\n",
    "def _estimate_tokens(text: str) -> int:\n",
    "\n",
    "    if not text:\n",
    "        return 0\n",
    "    return max(1, int(len(text) / 4))\n",
    "\n",
    "def _extract_usage(resp) -> Dict[str, Any]:\n",
    "\n",
    "    usage = getattr(resp, \"usage\", None)\n",
    "    if isinstance(usage, dict) and usage:\n",
    "        return usage\n",
    "\n",
    "    for attr in (\"response_metadata\", \"additional_kwargs\", \"metadata\"):\n",
    "        md = getattr(resp, attr, None)\n",
    "        if isinstance(md, dict) and md:\n",
    "\n",
    "            for k in (\"token_usage\", \"usage\"):\n",
    "                if k in md and isinstance(md[k], dict):\n",
    "                    return md[k]\n",
    "\n",
    "            if any(x in md for x in (\"prompt_tokens\", \"completion_tokens\", \"total_tokens\")):\n",
    "                return {\n",
    "                    \"prompt_tokens\": md.get(\"prompt_tokens\"),\n",
    "                    \"completion_tokens\": md.get(\"completion_tokens\"),\n",
    "                    \"total_tokens\": md.get(\"total_tokens\"),\n",
    "                }\n",
    "\n",
    "    return {}\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self, tpm_limit: int = 0, window_seconds: int = 60):\n",
    "        self.tpm_limit = int(tpm_limit or 0)\n",
    "        self.window_seconds = int(window_seconds)\n",
    "        self._events = deque()  # (timestamp, total_tokens)\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_tokens = 0\n",
    "        self.estimated_calls = 0\n",
    "        self.total_calls = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self._events.clear()\n",
    "        self.prompt_tokens = 0\n",
    "        self.completion_tokens = 0\n",
    "        self.total_tokens = 0\n",
    "        self.estimated_calls = 0\n",
    "        self.total_calls = 0\n",
    "\n",
    "    def _prune(self, now: float):\n",
    "        cutoff = now - self.window_seconds\n",
    "        while self._events and self._events[0][0] < cutoff:\n",
    "            self._events.popleft()\n",
    "\n",
    "    def tpm(self) -> int:\n",
    "        now = time.time()\n",
    "        self._prune(now)\n",
    "        return int(sum(t for _, t in self._events))\n",
    "\n",
    "    def record(self, *, prompt: str, completion: str, resp=None) -> Dict[str, Any]:\n",
    "\n",
    "        self.total_calls += 1\n",
    "\n",
    "        usage = _extract_usage(resp) if resp is not None else {}\n",
    "        p = usage.get(\"prompt_tokens\")\n",
    "        c = usage.get(\"completion_tokens\")\n",
    "        t = usage.get(\"total_tokens\")\n",
    "\n",
    "        estimated = False\n",
    "\n",
    "        if t is None:\n",
    "            p = _estimate_tokens(prompt) if p is None else int(p)\n",
    "            c = _estimate_tokens(completion) if c is None else int(c)\n",
    "            t = int(p) + int(c)\n",
    "            estimated = True\n",
    "            self.estimated_calls += 1\n",
    "        else:\n",
    "\n",
    "            t = int(t)\n",
    "            p = int(p) if p is not None else None\n",
    "            c = int(c) if c is not None else None\n",
    "\n",
    "        if p is not None:\n",
    "            self.prompt_tokens += int(p)\n",
    "        if c is not None:\n",
    "            self.completion_tokens += int(c)\n",
    "        self.total_tokens += int(t)\n",
    "\n",
    "        now = time.time()\n",
    "        self._events.append((now, int(t)))\n",
    "        self._prune(now)\n",
    "\n",
    "        return {\n",
    "            \"prompt_tokens\": p,\n",
    "            \"completion_tokens\": c,\n",
    "            \"total_tokens\": t,\n",
    "            \"estimated\": estimated,\n",
    "        }\n",
    "\n",
    "    def throttle_if_needed(self):\n",
    "        if not self.tpm_limit:\n",
    "            return\n",
    "        target = int(self.tpm_limit * TPM_SAFETY)\n",
    "        while self.tpm() > target:\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        return (\n",
    "            f\"calls={self.total_calls} | total_tokens={self.total_tokens} \"\n",
    "            f\"(prompt={self.prompt_tokens}, completion={self.completion_tokens}) | \"\n",
    "            f\"tpm_last_60s={self.tpm()} | estimated_calls={self.estimated_calls}\"\n",
    "        )\n",
    "\n",
    "TOKEN_TRACKER = TokenTracker(tpm_limit=TPM_LIMIT)\n",
    "\n",
    "def invoke_with_backoff(llm, prompt: str, max_retries: int = 6):\n",
    "    delay = 2.0\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            msg = str(e).lower()\n",
    "            if (\"rate limit\" in msg) or (\"429\" in msg) or (\"rate_limit\" in msg):\n",
    "                time.sleep(delay)\n",
    "                delay = min(delay * 2.0, 60.0)\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(\"Too many rate-limit retries (429).\")\n"
   ],
   "id": "5cf3946d3e47c57b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:07:59.040167Z",
     "start_time": "2026-01-11T14:07:59.031483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_ablation_next_n(\n",
    "    *,\n",
    "    llm,\n",
    "    doc_chunks: List[Any],\n",
    "    base_variant: PromptVariant,\n",
    "    topics_per_chunk: int,\n",
    "    keywords_per_topic: int,\n",
    "    final_k_topics: int,\n",
    "    output_dir: Path,\n",
    "    n: int = 2,\n",
    ") -> pd.DataFrame:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    TOKEN_TRACKER.reset()\n",
    "\n",
    "    variants = generate_ablation_variants_from_base(base_variant)\n",
    "    done = load_done(output_dir)\n",
    "\n",
    "    todo = [v for v in variants if v.variant_id not in done][:n]\n",
    "    if not todo:\n",
    "        print(\"All ablation variants already completed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for v in todo:\n",
    "        start_tokens = TOKEN_TRACKER.total_tokens\n",
    "        start_prompt = TOKEN_TRACKER.prompt_tokens\n",
    "        start_completion = TOKEN_TRACKER.completion_tokens\n",
    "        per_chunk_topics: List[List[Dict[str, Any]]] = []\n",
    "        parse_failures = 0\n",
    "\n",
    "        for chunk in doc_chunks:\n",
    "            topics = extract_topics_for_chunk_factorised(\n",
    "                llm=llm,\n",
    "                variant=v,\n",
    "                chunk=chunk,\n",
    "                topics_per_chunk=topics_per_chunk,\n",
    "                keywords_per_topic=keywords_per_topic,\n",
    "            )\n",
    "            if not topics:\n",
    "                parse_failures += 1\n",
    "            per_chunk_topics.append(topics)\n",
    "\n",
    "        merged = merge_topics_frequency_based(\n",
    "            per_chunk_topics=per_chunk_topics,\n",
    "            final_k_topics=final_k_topics,\n",
    "            keywords_per_topic=keywords_per_topic,\n",
    "        )\n",
    "\n",
    "\n",
    "        token_usage = {\n",
    "            \"prompt_tokens\": int(TOKEN_TRACKER.prompt_tokens),\n",
    "            \"completion_tokens\": int(TOKEN_TRACKER.completion_tokens),\n",
    "            \"total_tokens\": int(TOKEN_TRACKER.total_tokens),\n",
    "            \"tpm_last_60s\": int(TOKEN_TRACKER.tpm()),\n",
    "            \"total_calls\": int(TOKEN_TRACKER.total_calls),\n",
    "            \"estimated_calls\": int(TOKEN_TRACKER.estimated_calls),\n",
    "        }\n",
    "\n",
    "        run = {\n",
    "            \"stage\": \"ablation_extraction_only\",\n",
    "            \"variant_id\": v.variant_id,\n",
    "            \"num_chunks\": len(doc_chunks),\n",
    "            \"topics_per_chunk\": topics_per_chunk,\n",
    "            \"keywords_per_topic\": keywords_per_topic,\n",
    "            \"final_k_topics\": final_k_topics,\n",
    "            \"token_usage\": token_usage,\n",
    "            \"parse_failures\": int(parse_failures),\n",
    "            \"num_final_topics\": int(len(merged)),\n",
    "            \"components\": {\n",
    "                \"role\": bool(v.components.role),\n",
    "                \"rubric\": bool(v.components.rubric),\n",
    "                \"exemplars\": int(len(v.components.exemplars)),\n",
    "                \"delimiter\": bool(getattr(v.components.delimiter, \"open\", \"\") or getattr(v.components.delimiter, \"close\", \"\")),\n",
    "            },\n",
    "            \"topics\": merged,\n",
    "            \"per_chunk_topics\": per_chunk_topics,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": TOKEN_TRACKER.prompt_tokens - start_prompt,\n",
    "                \"completion_tokens\": TOKEN_TRACKER.completion_tokens - start_completion,\n",
    "                \"total_tokens\": TOKEN_TRACKER.total_tokens - start_tokens,\n",
    "                \"tpm_last_60s_end\": TOKEN_TRACKER.tpm(),\n",
    "                \"estimated_calls\": TOKEN_TRACKER.estimated_calls,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        ts = utc_stamp()\n",
    "        out_path = output_dir / f\"{ts}_{v.variant_id}.json\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(run, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        done.add(v.variant_id)\n",
    "        save_done(output_dir, done)\n",
    "\n",
    "        print(f\"Token usage ({v.variant_id}): {TOKEN_TRACKER.summary()}\")\n",
    "\n",
    "        rows.append({\n",
    "            \"variant_id\": v.variant_id,\n",
    "            \"parse_failures\": int(parse_failures),\n",
    "            \"num_final_topics\": int(len(merged)),\n",
    "            \"components\": {\n",
    "                \"role\": bool(v.components.role),\n",
    "                \"rubric\": bool(v.components.rubric),\n",
    "                \"exemplars\": int(len(v.components.exemplars)),\n",
    "                \"delimiter\": bool(getattr(v.components.delimiter, \"open\", \"\") or getattr(v.components.delimiter, \"close\", \"\")),\n",
    "            },\n",
    "            \"out_file\": str(out_path),\n",
    "            \"status\": \"saved\",\n",
    "            \"total_tokens\": int(TOKEN_TRACKER.total_tokens),\n",
    "            \"tpm_last_60s\": int(TOKEN_TRACKER.tpm()),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    manifest_path = output_dir / \"manifest.csv\"\n",
    "    if manifest_path.exists():\n",
    "        old = pd.read_csv(manifest_path)\n",
    "        all_rows = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_rows = df\n",
    "\n",
    "    all_rows.to_csv(manifest_path, index=False)\n",
    "\n",
    "    return df.sort_values([\"parse_failures\", \"variant_id\"], ascending=[True, True])"
   ],
   "id": "16dad8e2bff4b7be",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T13:08:09.027851Z",
     "start_time": "2026-01-10T13:06:04.583411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = Path(\"outputs/ablation_runs\")\n",
    "\n",
    "manifest_today = run_ablation_next_n(\n",
    "    llm=llm,\n",
    "    doc_chunks=doc_chunk,\n",
    "    base_variant=base_variant,\n",
    "    topics_per_chunk=TOPICS_PER_CHUNK,\n",
    "    keywords_per_topic=KEYWORDS_PER_TOPIC,\n",
    "    final_k_topics=FINAL_K_TOPICS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    n=1,\n",
    ")\n",
    "manifest_today"
   ],
   "id": "2dcfae5b26e7575b",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Too many rate-limit retries (429).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m OUTPUT_DIR = Path(\u001B[33m\"\u001B[39m\u001B[33moutputs/ablation_runs\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m manifest_today = \u001B[43mrun_ablation_next_n\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdoc_chunks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdoc_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_variant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_variant\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTOPICS_PER_CHUNK\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m=\u001B[49m\u001B[43mKEYWORDS_PER_TOPIC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfinal_k_topics\u001B[49m\u001B[43m=\u001B[49m\u001B[43mFINAL_K_TOPICS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mOUTPUT_DIR\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     13\u001B[39m manifest_today\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mrun_ablation_next_n\u001B[39m\u001B[34m(llm, doc_chunks, base_variant, topics_per_chunk, keywords_per_topic, final_k_topics, output_dir, n)\u001B[39m\n\u001B[32m     33\u001B[39m parse_failures = \u001B[32m0\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m doc_chunks:\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     topics = \u001B[43mextract_topics_for_chunk_factorised\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[43m=\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvariant\u001B[49m\u001B[43m=\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtopics_per_chunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeywords_per_topic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     43\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m topics:\n\u001B[32m     44\u001B[39m         parse_failures += \u001B[32m1\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mextract_topics_for_chunk_factorised\u001B[39m\u001B[34m(llm, variant, chunk, topics_per_chunk, keywords_per_topic)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m MIN_SLEEP_SECONDS:\n\u001B[32m     34\u001B[39m     time.sleep(MIN_SLEEP_SECONDS)\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m resp = \u001B[43minvoke_with_backoff\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m content = \u001B[38;5;28mgetattr\u001B[39m(resp, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mstr\u001B[39m(resp))\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Record token usage (exact when available; otherwise estimated)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 159\u001B[39m, in \u001B[36minvoke_with_backoff\u001B[39m\u001B[34m(llm, prompt, max_retries)\u001B[39m\n\u001B[32m    157\u001B[39m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mToo many rate-limit retries (429).\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mRuntimeError\u001B[39m: Too many rate-limit retries (429)."
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:27:17.977695Z",
     "start_time": "2026-01-11T14:27:17.964460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUT_DIR = Path(\"outputs/ablation_runs\")\n",
    "\n",
    "def is_run_file(p: Path) -> bool:\n",
    "    if p.suffix.lower() != \".json\":\n",
    "        return False\n",
    "    if p.name in {\"done_variants.json\"}:\n",
    "        return False\n",
    "    if p.name.endswith(\"_PARTIAL.json\"):\n",
    "        return False\n",
    "    try:\n",
    "        obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return False\n",
    "    return isinstance(obj, dict) and (\"variant_id\" in obj) and (\"topics\" in obj)\n",
    "\n",
    "run_files = [p for p in OUTPUT_DIR.iterdir() if is_run_file(p)]\n",
    "run_files = sorted(run_files, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "\n",
    "if not run_files:\n",
    "    print(\"No run files found in\", OUTPUT_DIR)\n",
    "else:\n",
    "    p = run_files[0]\n",
    "    run = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    print(\"File:\", p.name)\n",
    "    print(\"Variant:\", run.get(\"variant_id\"))\n",
    "    print(\"Parse failures:\", run.get(\"parse_failures\"))\n",
    "    print(\"Final topics:\", len(run.get(\"topics\", [])))\n",
    "    print(\"Example topic:\", (run.get(\"topics\") or [None])[0])\n",
    "    print(\"Token usage:\", run.get(\"token_usage\"))"
   ],
   "id": "321a3326b9f66f29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 20260110_123744_abl_minus_role.json\n",
      "Variant: abl_minus_role\n",
      "Parse failures: 0\n",
      "Final topics: 10\n",
      "Example topic: {'label': 'Competitive Landscape', 'keywords': ['market share', 'competition', 'market rivals', 'rival offerings', 'competitive dynamics', 'competitive intensity', 'competitors', 'integrated solutions'], 'support_chunks': 5}\n",
      "Token usage: {'prompt_tokens': 85363, 'completion_tokens': 105715, 'total_tokens': 191078, 'tpm_last_60s_end': 14898, 'estimated_calls': 0}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-11T14:39:26.868518Z",
     "start_time": "2026-01-11T14:39:26.865658Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "670d911155b4f946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stage', 'variant_id', 'num_chunks', 'topics_per_chunk', 'keywords_per_topic', 'final_k_topics', 'token_usage', 'parse_failures', 'num_final_topics', 'components', 'topics', 'per_chunk_topics'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "175bfe04c41dc4fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
